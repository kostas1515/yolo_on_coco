{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import cv2\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='labels/coco2017labels.zip'\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "#     bbox=zip.read('coco/labels/train2017/000000371735.txt')\n",
    "#     box=(bbox.decode(\"utf-8\"))\n",
    "#     box=pd.DataFrame([x.split() for x in box.rstrip('\\n').split('\\n')],columns=['class','xc','yc','w','h'])\n",
    "#     print(box)\n",
    "    count = len(zip.infolist())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name='labels/coco2017labels.zip'\n",
    "zip_file = ZipFile(file_name)\n",
    "\n",
    "dfs = {text_file.filename: pd.read_csv(zip_file.open(text_file.filename))\n",
    "       for text_file in zip_file.infolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a=torch.tensor([[[1,2,2,4,0,0,0,1],[2,3,5,3,0,0,1,0]]])\n",
    "print(a.shape)\n",
    "b=((a[:,:,4:]==1).nonzero())\n",
    "b=b[:,-1].unsqueeze(0).unsqueeze(-1)\n",
    "print(b.shape)\n",
    "new=a[:,:,:4]\n",
    "print(new.shape)\n",
    "new=torch.cat((new.T,b.T)).T\n",
    "print(new)\n",
    "sorted_pred=torch.sort(new[0,:,4],descending=False)\n",
    "print(sorted_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = box.values.astype(np.float32)\n",
    "b=torch.tensor(b)\n",
    "labels = b.T[0].reshape(b.shape[0], 1)\n",
    "one_hot_target = (labels == torch.arange(80).reshape(1, 80)).float()\n",
    "conf=torch.ones([b.shape[0],1])\n",
    "boxes=torch.cat((b.T[1:],conf.T,one_hot_target.T)).T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='images/train2017.zip'\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    image=zip.read('train2017/000000408542.jpg')\n",
    "    img = cv2.imdecode(np.frombuffer(image, np.uint8),1)\n",
    "    im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    io.imshow(im_rgb)\n",
    "    io.imsave('fig.png',im_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('pointers/train2017.txt',names=['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['box']=df['filename'].apply(lambda x: 'coco/labels/'+x.split('.')[0]+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Length of dataset is 118287\n",
      "\n",
      "\n",
      " epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/site-packages/torch/nn/functional.py:2503: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pgr:8.210538774336994% L:44.853084564208984 IoU:0.7186965942382812 pob:0.4280465245246887 nob:0.003420119872316718 PCls:0.40294158458709717 ncls:0.027211023494601257643"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ef2dc7550ad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtrue_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0miou_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoobj_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_responsible_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dataset import *\n",
    "import timeit \n",
    "import cv2\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from darknet import *\n",
    "import darknet as dn\n",
    "import util as util\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import timeit\n",
    "import torch.autograd\n",
    "import helper as helper\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "net = Darknet(\"../cfg/yolov3.cfg\")\n",
    "inp_dim=net.inp_dim\n",
    "pw_ph=net.pw_ph.to(device='cuda')\n",
    "cx_cy=net.cx_cy.to(device='cuda')\n",
    "stride=net.stride.to(device='cuda')\n",
    "\n",
    "\n",
    "'''\n",
    "when loading weights from dataparallel model then, you first need to instatiate the dataparallel model \n",
    "if you start fresh then first model.load_weights and then make it parallel\n",
    "'''\n",
    "try:\n",
    "    PATH = '../bcelogit.pth'\n",
    "    weights = torch.load(PATH)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we https://pytorch.org/docs/stable/data.html#torch.utils.data.Datasetare on a CUDA machine, this should print a CUDA device:\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 9:\n",
    "        print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(net)\n",
    "        model.to(device)\n",
    "        model.load_state_dict(weights)\n",
    "    else:\n",
    "        model=net\n",
    "        model.to(device)\n",
    "        model.load_state_dict(weights)\n",
    "        \n",
    "except FileNotFoundError: \n",
    "#     net.load_weights(\"yolov3.weights\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(net)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        model=net\n",
    "        \n",
    "        \n",
    "transformed_dataset=Coco(partition='train',\n",
    "                                           transform=transforms.Compose([\n",
    "                                            ResizeToTensor(inp_dim)\n",
    "                                           ]))\n",
    "\n",
    "\n",
    "writer = SummaryWriter('../results/test')\n",
    "dataset_len=(len(transformed_dataset))\n",
    "print('Length of dataset is '+ str(dataset_len)+'\\n')\n",
    "batch_size=8\n",
    "\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                        shuffle=True,collate_fn=helper.my_collate, num_workers=2)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, weight_decay=0.0005, momentum=0.9)\n",
    "epochs=50\n",
    "total_loss=0\n",
    "write=0\n",
    "misses=0\n",
    "break_flag=0\n",
    "avg_iou=0\n",
    "for e in range(epochs):\n",
    "    prg_counter=0\n",
    "    train_counter=0\n",
    "    total_loss=0\n",
    "    avg_iou=0\n",
    "    avg_infs=0\n",
    "    avg_conf=0\n",
    "    avg_no_conf=0\n",
    "    avg_pos=0\n",
    "    avg_neg=0\n",
    "    print(\"\\n epoch \"+str(e))\n",
    "    misses=0\n",
    "    for images,targets,img_names in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        targets,anchors,offset,strd,mask=helper.collapse_boxes(targets,pw_ph,cx_cy,stride)\n",
    "        images=images.cuda()\n",
    "        raw_pred = model(images, torch.cuda.is_available())\n",
    "        raw_pred=helper.expand_predictions(raw_pred,mask)\n",
    "        true_pred=util.transform(raw_pred.clone(),anchors,offset,strd)\n",
    "        targets=targets.unsqueeze(-3)\n",
    "        targets=targets.cuda()\n",
    "        iou_mask,noobj_mask=util.get_responsible_masks(true_pred,targets,offset,strd,mask,inp_dim)\n",
    "        \n",
    "        iou1=torch.diag(util.bbox_iou(util.get_abs_coord(true_pred[iou_mask.T,:].unsqueeze(-3)),util.transpose_target(get_abs_coord(targets)*inp_dim)))\n",
    "        iou=iou1.mean().item()\n",
    "        noobj_box=raw_pred[:,:,4:5].clone()\n",
    "        noobj_box=helper.uncollapse(noobj_box,mask)\n",
    "        noobj_mask=helper.uncollapse(noobj_mask.T.unsqueeze(-1),mask)\n",
    "        \n",
    "        \n",
    "        conf=raw_pred[iou_mask.T,:][:,4:5].mean().item()\n",
    "        class_mask=targets[:,:,5:].type(torch.BoolTensor).squeeze(0)\n",
    "        if(iou_mask.sum()==class_mask.shape[0]):\n",
    "            pos_class=raw_pred[iou_mask.T,:][:,5:][class_mask].mean().item()\n",
    "            neg_class=raw_pred[iou_mask.T,:][:,5:][~class_mask].mean().item()\n",
    "        else:\n",
    "            pos_class=0\n",
    "            neg_class=0\n",
    "        noobj_box=noobj_box[noobj_mask]\n",
    "        no_obj_conf=noobj_box.mean().item()\n",
    "        \n",
    "        raw_pred=raw_pred[iou_mask.T,:]\n",
    "        anchors=anchors[iou_mask.T,:]\n",
    "        offset=offset[iou_mask.T,:]\n",
    "        strd=strd[iou_mask.T,:]\n",
    "        if(strd.shape[0]==sum(mask)):#this means that iou_mask failed and was all true, because max of zeros is true for all lenght of mask strd\n",
    "            targets[:,:,0:4]=targets[:,:,0:4]*inp_dim\n",
    "            targets=targets.squeeze(0)\n",
    "            targets[:,0:4]=util.transform_groundtruth(targets,anchors,offset,strd)\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                loss=util.yolo_loss(raw_pred,targets,noobj_box,mask)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            avg_conf=avg_conf+conf\n",
    "            avg_no_conf=avg_no_conf+no_obj_conf\n",
    "            avg_pos=avg_pos+pos_class\n",
    "            avg_neg=avg_neg+neg_class\n",
    "            total_loss=total_loss+loss.item()\n",
    "            avg_iou=avg_iou+iou\n",
    "            sys.stdout.write('\\rPgr:'+str(prg_counter/dataset_len*100*batch_size)+'%' ' L:'+ str(loss.item()))\n",
    "            sys.stdout.write(' IoU:' +str(iou)+' pob:'+str(conf)+ ' nob:'+str(no_obj_conf))\n",
    "            sys.stdout.write(' PCls:' +str(pos_class)+' ncls:'+str(neg_class))\n",
    "            sys.stdout.flush()\n",
    "            prg_counter=prg_counter+1\n",
    "            train_counter=train_counter+1\n",
    "        else:\n",
    "            print('missed')\n",
    "            print(strd.shape[0])\n",
    "            prg_counter=prg_counter+1\n",
    "        iou1=iou1.tolist()\n",
    "        iou_per_class= [0] * 80\n",
    "        averager_per_class=[1] * 80\n",
    "        for i,el in enumerate(iou1):\n",
    "            name=targets[i,5:].max(0)[1].cpu().detach().numpy()\n",
    "            iou_per_class[name]=iou_per_class[name]+el\n",
    "            averager_per_class[name]=averager_per_class[name]+1\n",
    "        for i in range(80):\n",
    "            if (iou_per_class[i]!=0):\n",
    "                if i<40:\n",
    "                    writer.add_scalar('Iou0/'+str(i), iou_per_class[i]/averager_per_class[i], train_counter)\n",
    "                else:\n",
    "                    writer.add_scalar('Iou1/'+str(i), iou_per_class[i]/averager_per_class[i], train_counter)\n",
    "        writer.add_scalar('AvLoss/train', total_loss/train_counter, train_counter)\n",
    "\n",
    "        \n",
    "        writer.add_scalar('AvIoU/train', avg_iou/train_counter, train_counter)\n",
    "\n",
    "        writer.add_scalar('AvPConf/train', avg_conf/train_counter, train_counter)\n",
    "        \n",
    "        writer.add_scalar('AvNConf/train', avg_no_conf/train_counter, train_counter)\n",
    "        \n",
    "        writer.add_scalar('AvClass/train', avg_pos/train_counter, train_counter)\n",
    "        \n",
    "        writer.add_scalar('AvNClass/train', avg_neg/train_counter, train_counter)\n",
    "        \n",
    "        del loss, raw_pred, targets, true_pred, images,iou,noobj_box,conf,iou1\n",
    "        torch.cuda.empty_cache()\n",
    "    if misses>0:\n",
    "        break\n",
    "#     torch.save(model.state_dict(), PATH)\n",
    "#     writer.add_scalar('Loss/train', total_loss/train_counter, e)\n",
    "#     writer.add_scalar('AIoU/train', avg_iou/train_counter, e)\n",
    "#     writer.add_scalar('PConf/train', avg_conf/train_counter, e)\n",
    "#     writer.add_scalar('NConf/train', avg_no_conf/train_counter, e)\n",
    "#     writer.add_scalar('PClass/train', avg_pos/train_counter, e)\n",
    "#     writer.add_scalar('NClass/train', avg_neg/train_counter, e)\n",
    "    print('\\ntotal number of misses is ' + str(misses))\n",
    "    print('\\n total average loss is '+str(total_loss/train_counter))\n",
    "    print('\\n total average iou is '+str(avg_iou/train_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mxnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f44c18cf54e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograd\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mxnet'"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "import mxnet\n",
    "from gluoncv import model_zoo, data, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([41.,  0.,  9.,  0.,  0.,  0.,  1.,  4.,  5.,  0.,  0.,  1.,  0.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  6.,  3.,  0.,  0.,\n",
      "         0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.])\n",
      "[21, 26, 1, 5, 4, 21, 1, 1]\n",
      "80\n",
      "tensor([ 0,  0,  0,  0,  0,  0, 30,  0,  0,  0,  8,  8,  8, 24,  0,  8,  0,  0,\n",
      "         8, 24,  0,  2,  2,  2,  2,  7,  0,  0,  0,  0,  0,  0,  0, 33,  0,  0,\n",
      "         2,  2,  7, 13,  0,  2,  7,  7,  0,  0,  0, 11,  2,  0,  0,  2, 36, 42,\n",
      "        43, 53, 60, 25, 25, 25, 29, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,\n",
      "         0,  0,  0, 24, 24,  0, 75,  6])\n",
      "tensor([0.6685, 0.0000, 2.1848, 0.0000, 0.0000, 0.0000, 4.3820, 2.9957, 2.7726,\n",
      "        0.0000, 0.0000, 4.3820, 0.0000, 4.3820, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.5903, 3.2834, 0.0000,\n",
      "        0.0000, 0.0000, 4.3820, 4.3820, 0.0000, 0.0000, 4.3820, 0.0000, 0.0000,\n",
      "        4.3820, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.3820, 4.3820, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.3820,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.3820, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 4.3820, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.6685, 0.6685, 0.6685, 0.6685, 0.6685, 0.6685, 4.3820, 0.6685, 0.6685,\n",
      "        0.6685, 2.7726, 2.7726, 2.7726, 2.5903, 0.6685, 2.7726, 0.6685, 0.6685,\n",
      "        2.7726, 2.5903, 0.6685, 2.1848, 2.1848, 2.1848, 2.1848, 2.9957, 0.6685,\n",
      "        0.6685, 0.6685, 0.6685, 0.6685, 0.6685, 0.6685, 4.3820, 0.6685, 0.6685,\n",
      "        2.1848, 2.1848, 2.9957, 4.3820, 0.6685, 2.1848, 2.9957, 2.9957, 0.6685,\n",
      "        0.6685, 0.6685, 4.3820, 2.1848, 0.6685, 0.6685, 2.1848, 4.3820, 4.3820,\n",
      "        4.3820, 4.3820, 4.3820, 3.2834, 3.2834, 3.2834, 4.3820, 2.5903, 0.6685,\n",
      "        0.6685, 0.6685, 0.6685, 0.6685, 0.6685, 0.6685, 0.6685, 0.6685, 2.5903,\n",
      "        0.6685, 0.6685, 0.6685, 2.5903, 2.5903, 0.6685, 4.3820, 4.3820])\n",
      "tensor([0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
      "        0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
      "        0.0476, 0.0476, 0.0476, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385,\n",
      "        0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385,\n",
      "        0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385,\n",
      "        0.0385, 0.0385, 1.0000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2500,\n",
      "        0.2500, 0.2500, 0.2500, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
      "        0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,\n",
      "        0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(targets[:,5:].sum(axis=0))\n",
    "print(mask)\n",
    "print(sum(mask))\n",
    "classes=targets[:,5:].max(1)[1]\n",
    "print(classes)\n",
    "idf=torch.log(sum(mask)/targets[:,5:].sum(axis=0))\n",
    "idf[idf== float('inf')] = 0\n",
    "print(idf)\n",
    "print(idf[classes])\n",
    "\n",
    "tf=[1/mask[i] for i in range(len(mask)) for j in range(mask[i])]\n",
    "tf=torch.tensor([1/mask[i] for i in range(len(mask)) for j in range(mask[i])])\n",
    "print(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets2,anchors,offset,strd,mask=helper.collapse_boxes(targets,pw_ph,cx_cy,stride)\n",
    "transformed_output=true_pred\n",
    "targets=targets\n",
    "offset=offset\n",
    "strd=strd\n",
    "mask=mask\n",
    "inp_dim\n",
    "'''\n",
    "this function takes the transformed_output and\n",
    "the target box in respect to the resized image size\n",
    "and returns a mask which can be applied to select the \n",
    "best raw input,anchors and cx_cy_offset\n",
    "and the noobj_mask for the negatives\n",
    "targets is a list\n",
    "'''\n",
    "#first transpose the centered normalised target coords\n",
    "centered_target=transpose_target(targets)[:,:,0:2]\n",
    "#multiply by inp_dim then devide by stride to get the relative grid size coordinates, floor the result to get the corresponding cell\n",
    "centered_target=torch.floor(centered_target*inp_dim/strd)\n",
    "#create a mask to find where the gt falls into which gridcell in the grid coordinate system\n",
    "fall_into_mask=centered_target==offset\n",
    "fall_into_mask=fall_into_mask[:,:,0]&fall_into_mask[:,:,1]\n",
    "#     fall_into_mask= ~fall_into_mask\n",
    "#create a copy of the transformed output\n",
    "best_bboxes=transformed_output.clone()\n",
    "#apply reverse mask to copy in order to zero all other bbox locations\n",
    "best_bboxes[~fall_into_mask]=0   \n",
    "#transform the copy to xmin,xmax,ymin,ymax\n",
    "best_responsible_coord=get_abs_coord(best_bboxes)\n",
    "targets=transpose_target(get_abs_coord(targets))*inp_dim\n",
    "#calculate best iou and mask\n",
    "responsible_iou=bbox_iou(best_responsible_coord,targets,True)\n",
    "\n",
    "responsible_iou[responsible_iou.ne(responsible_iou)] = 0\n",
    "responsible_mask=responsible_iou.max(dim=0)[0] == responsible_iou\n",
    "\n",
    "print(responsible_mask.shape)\n",
    "\n",
    "abs_coord=get_abs_coord(transformed_output)\n",
    "iou=bbox_iou(abs_coord,targets,True)\n",
    "iou[iou.ne(iou)] = 0\n",
    "ignore_mask=0.5<=iou\n",
    "inverted_mask=iou.max(dim=0)[0] != iou\n",
    "noobj_mask=~same_picture_mask(responsible_mask.clone()|ignore_mask,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(responsible_mask)\n",
    "if(responsible_mask.sum()>sum(mask)):\n",
    "    print('jello')\n",
    "    responsible_mask1=correct_iou_mask(responsible_mask,fall_into_mask)\n",
    "print(responsible_mask1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.randint(0, 9, (1,)) == torch.arange(9)\n",
    "a=a.repeat(5)\n",
    "a=a.repeat(2,1)\n",
    "print(a)\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_mask=(responsible_mask.sum(axis=0)==responsible_mask.sum(axis=0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsible_mask[:,le_mask]=(responsible_mask[:,le_mask]&fall_into_mask[le_mask,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(le_mask)\n",
    "print(fall_into_mask[le_mask,:].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([1,2,3,4,5,6,6,6])\n",
    "indices=(((a==a.max())==True).nonzero())\n",
    "for ind in indices:\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "testing with ../bcelogit.pth\n",
      "\n",
      "Length of dataset is 5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/site-packages/torch/nn/functional.py:2503: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    }
   ],
   "source": [
    "from darknet import *\n",
    "import darknet as dn\n",
    "import util as util\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import timeit\n",
    "from dataset import *\n",
    "import torchvision.ops.boxes as nms_box\n",
    "import helper as helper\n",
    "from utils import *\n",
    "from BoundingBox import BoundingBox\n",
    "from BoundingBoxes import BoundingBoxes\n",
    "\n",
    "\n",
    "net = Darknet(\"../cfg/yolov3.cfg\")\n",
    "inp_dim=net.inp_dim\n",
    "pw_ph=net.pw_ph\n",
    "cx_cy=net.cx_cy\n",
    "stride=net.stride\n",
    "\n",
    "\n",
    "'''\n",
    "when loading weights from dataparallel model then, you first need to instatiate the dataparallel model \n",
    "if you start fresh then first model.load_weights and then make it parallel\n",
    "'''\n",
    "try:\n",
    "    PATH = '../bcelogit.pth'\n",
    "    weights = torch.load(PATH)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 9:\n",
    "        print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(net)\n",
    "    else:\n",
    "        model=net\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "except FileNotFoundError: \n",
    "    net.load_weights(\"../yolov3.weights\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "      print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "      model = nn.DataParallel(net)\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('testing with '+ PATH +'\\n')\n",
    "transformed_dataset=Coco(partition='val',\n",
    "                                           transform=transforms.Compose([\n",
    "                                            ResizeToTensor(inp_dim)\n",
    "                                           ]))\n",
    "\n",
    "\n",
    "\n",
    "dataset_len=(len(transformed_dataset))\n",
    "print('Length of dataset is '+ str(dataset_len)+'\\n')\n",
    "batch_size=8\n",
    "\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                        shuffle=True,collate_fn=helper.my_collate, num_workers=2)\n",
    "\n",
    "true_pos=0\n",
    "false_pos=0\n",
    "counter=0\n",
    "iou_threshold=0.5\n",
    "confidence=0.9\n",
    "recall_counter=0\n",
    "\n",
    "for images,targets,img_name in dataloader:\n",
    "    inp=images.cuda()\n",
    "    raw_pred = model(inp, torch.cuda.is_available())\n",
    "    targets,anchors,offset,strd,mask=helper.collapse_boxes(targets,pw_ph,cx_cy,stride)\n",
    "    raw_pred=raw_pred.to(device='cuda')\n",
    "    true_pred=util.transform(raw_pred.clone(),pw_ph,cx_cy,stride)\n",
    "    \n",
    "    sorted_pred=torch.sort(true_pred[:,:,4],descending=True)\n",
    "    pred_mask=sorted_pred[0]>confidence\n",
    "    indices=[(sorted_pred[1][e,:][pred_mask[e,:]]) for e in range(pred_mask.shape[0])]\n",
    "    pred_final=[true_pred[i,indices[i],:] for i in range(len(indices))]\n",
    "    \n",
    "    pred_final_coord=[util.get_abs_coord(pred_final[i].unsqueeze(-2)) for i in range(len(pred_final))]\n",
    "    \n",
    "    indices=[nms_box.nms(pred_final_coord[i][0],pred_final[i][:,4],iou_threshold) for i in range(len(pred_final))]\n",
    "\n",
    "    pred_final=[pred_final[i][indices[i],:] for i in range(len(pred_final))]\n",
    "#     pred_final[:,0:4]=pred_final[:,0:4]/inp_dim\n",
    "    helper.write_pred(img_name,pred_final,inp_dim)\n",
    "    \n",
    "\n",
    "    \n",
    "# Read txt files containing bounding boxes (ground truth and detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darknet import *\n",
    "import darknet as dn\n",
    "import util as util\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import timeit\n",
    "from dataset import *\n",
    "import torchvision.ops.boxes as nms_box\n",
    "import helper as helper\n",
    "from utils import *\n",
    "from BoundingBox import BoundingBox\n",
    "from BoundingBoxes import BoundingBoxes\n",
    "boundingboxes = helper.getBoundingBoxes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COCO map is: 0.11912917886179519\n",
      "\n",
      "COCO map is: 0.2355674385310013\n",
      "\n",
      "COCO map is: 0.34915411713100675\n",
      "\n",
      "COCO map is: 0.4595668838647486\n",
      "\n",
      "COCO map is: 0.567306052302071\n",
      "\n",
      "COCO map is: 0.6718939752145684\n",
      "\n",
      "COCO map is: 0.7726393639891111\n",
      "\n",
      "COCO map is: 0.8695014772828528\n",
      "\n",
      "COCO map is: 0.9611930350544186\n",
      "\n",
      "COCO map is: 1.0465174780470763\n",
      "\n",
      "COCO map is: 1.1236037836382047\n",
      "\n",
      "COCO map is: 1.1909904569651755\n",
      "\n",
      "COCO map is: 1.245796372937909\n",
      "\n",
      "COCO map is: 1.287052263488623\n",
      "\n",
      "COCO map is: 1.315205077988326\n",
      "\n",
      "COCO map is: 1.3288947661636286\n",
      "\n",
      "COCO map is: 1.3339500201004177\n",
      "\n",
      "COCO map is: 1.3354027677688\n",
      "\n",
      "COCO map is: 1.335641862315929\n"
     ]
    }
   ],
   "source": [
    "from Evaluator import *\n",
    "from utils import *\n",
    "from BoundingBox import BoundingBox\n",
    "from BoundingBoxes import BoundingBoxes\n",
    "\n",
    "\n",
    "boundingboxes = helper.getBoundingBoxes()\n",
    "\n",
    "evaluator = Evaluator()\n",
    "\n",
    "iou=0.05\n",
    "MMap=0\n",
    "while iou<1:\n",
    "    metricsPerClass = evaluator.GetPascalVOCMetrics(boundingboxes, IOUThreshold=iou)\n",
    "#     print(\"Average precision values per class:\\n\")\n",
    "    # Loop through classes to obtain their metrics\n",
    "    mAP=0\n",
    "    counter=0\n",
    "    for mc in metricsPerClass:\n",
    "        # Get metric values per each class\n",
    "        c = mc['class']\n",
    "        precision = mc['precision']\n",
    "        recall = mc['recall']\n",
    "        average_precision = mc['AP']\n",
    "        ipre = mc['interpolated precision']\n",
    "        irec = mc['interpolated recall']\n",
    "        # Print AP per class\n",
    "        mAP=average_precision+mAP\n",
    "#         print('%s: %f' % (c, average_precision))\n",
    "    MMap=MMap+mAP/80\n",
    "    iou=iou+0.05\n",
    "    print('\\nCOCO map is:',MMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2716704055107972\n"
     ]
    }
   ],
   "source": [
    "print(MMap/19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9: 0.000231\n"
     ]
    }
   ],
   "source": [
    "print('%s: %f' % (c, average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f162f914160>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluator import *\n",
    "evaluator = Evaluator()\n",
    "metricsPerClass = evaluator.GetPascalVOCMetrics(boundingboxes, IOUThreshold=0.5)\n",
    "print(\"Average precision values per class:\\n\")\n",
    "# Loop through classes to obtain their metrics\n",
    "mAP=0\n",
    "counter=0\n",
    "for mc in metricsPerClass:\n",
    "    # Get metric values per each class\n",
    "    c = mc['class']\n",
    "    precision = mc['precision']\n",
    "    recall = mc['recall']\n",
    "    average_precision = mc['AP']\n",
    "    ipre = mc['interpolated precision']\n",
    "    irec = mc['interpolated recall']\n",
    "    # Print AP per class\n",
    "    mAP=average_precision+mAP\n",
    "    print('%s: %f' % (c, average_precision))\n",
    "\n",
    "print('map is:',mAP/80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.07290217209136615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "coord=pred_final[:,:4].cpu().detach().numpy()\n",
    "conf=pred_final[:,4:5].cpu().detach().numpy()\n",
    "mat=np.hstack((conf,coord))\n",
    "\n",
    "classes=pred_final[:,5:].max(1)[1].cpu().detach().numpy()\n",
    "classes=np.array([classes]).T\n",
    "\n",
    "mat=np.hstack((classes,mat))\n",
    "mat=np.array(mat)\n",
    "\n",
    "df=pd.DataFrame(mat,index=None,columns=None)\n",
    "df[0]=df[0].apply(lambda x: int(x))\n",
    "\n",
    "df.to_csv('test.txt',sep=' ',header=False,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('test.txt',mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([1,2,2,3,4,2,3,1,4])\n",
    "print(a.max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=(inp).squeeze(0)\n",
    "image=np.array(image.cpu())\n",
    "print(image.shape)\n",
    "image =  image[:,:,::-1].transpose((1,2,0))\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "all_files = glob.glob('plots/' + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in sorted(all_files):\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=1, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# gca stands for 'get current axis'\n",
    "\n",
    "title_list=['AIoU_train','Loss_train','NClass_train','NConf_train','PClass','PConf']\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "fig.suptitle('KL for xy loss')\n",
    "fig.subplots_adjust(hspace=0.3, wspace=-.6)\n",
    "colors = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c']\n",
    "i=2\n",
    "k=0\n",
    "while i <18:\n",
    "    ax = fig.add_subplot(2, 9, i)\n",
    "    frame.plot(x =1 , y = i,ax=ax,legend=False)\n",
    "    ax.set_title(title_list[k])\n",
    "    i=i+3\n",
    "    k=k+1\n",
    "plt.savefig('original.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
