{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import cv2\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='labels/coco2017labels.zip'\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "#     bbox=zip.read('coco/labels/train2017/000000371735.txt')\n",
    "#     box=(bbox.decode(\"utf-8\"))\n",
    "#     box=pd.DataFrame([x.split() for x in box.rstrip('\\n').split('\\n')],columns=['class','xc','yc','w','h'])\n",
    "#     print(box)\n",
    "    count = len(zip.infolist())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name='labels/coco2017labels.zip'\n",
    "zip_file = ZipFile(file_name)\n",
    "\n",
    "dfs = {text_file.filename: pd.read_csv(zip_file.open(text_file.filename))\n",
    "       for text_file in zip_file.infolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a=torch.tensor([[[1,2,2,4,0,0,0,1],[2,3,5,3,0,0,1,0]]])\n",
    "print(a.shape)\n",
    "b=((a[:,:,4:]==1).nonzero())\n",
    "b=b[:,-1].unsqueeze(0).unsqueeze(-1)\n",
    "print(b.shape)\n",
    "new=a[:,:,:4]\n",
    "print(new.shape)\n",
    "new=torch.cat((new.T,b.T)).T\n",
    "print(new)\n",
    "sorted_pred=torch.sort(new[0,:,4],descending=False)\n",
    "print(sorted_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = box.values.astype(np.float32)\n",
    "b=torch.tensor(b)\n",
    "labels = b.T[0].reshape(b.shape[0], 1)\n",
    "one_hot_target = (labels == torch.arange(80).reshape(1, 80)).float()\n",
    "conf=torch.ones([b.shape[0],1])\n",
    "boxes=torch.cat((b.T[1:],conf.T,one_hot_target.T)).T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='images/train2017.zip'\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    image=zip.read('train2017/000000408542.jpg')\n",
    "    img = cv2.imdecode(np.frombuffer(image, np.uint8),1)\n",
    "    im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    io.imshow(im_rgb)\n",
    "    io.imsave('fig.png',im_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('pointers/train2017.txt',names=['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['box']=df['filename'].apply(lambda x: 'coco/labels/'+x.split('.')[0]+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Using  2 GPUs!\n",
      "Length of dataset is 118287\n",
      "\n",
      "\n",
      " epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/site-packages/torch/nn/functional.py:2503: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pgr:99.9940821899279% L:2.1205382347106934 IoU:0.7564195990562439 pob:0.49523231387138367 nob:0.0006193502340465784 PCls:0.8977230787277222 ncls:0.00129464420024305581556\n",
      "total number of misses is 0\n",
      "\n",
      " total average loss is 2.113286115961792\n",
      "\n",
      " total average iou is 0.7594813029675462\n",
      "\n",
      " epoch 1\n",
      "Pgr:51.339538579894665% L:1.9521749019622803 IoU:0.7833840250968933 pob:0.5720711946487427 nob:0.00022058552713133395 PCls:0.9273592829704285 ncls:0.000919503334444016256"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-56ac2fcc4b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolo_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoobj_box\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/faster_rcnn/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/faster_rcnn/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dataset import *\n",
    "import timeit \n",
    "import cv2\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from darknet import *\n",
    "import darknet as dn\n",
    "import util as util\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import timeit\n",
    "import torch.autograd\n",
    "import helper as helper\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "net = Darknet(\"../cfg/yolov3.cfg\")\n",
    "inp_dim=net.inp_dim\n",
    "pw_ph=net.pw_ph.to(device='cuda')\n",
    "cx_cy=net.cx_cy.to(device='cuda')\n",
    "stride=net.stride.to(device='cuda')\n",
    "\n",
    "\n",
    "'''\n",
    "when loading weights from dataparallel model then, you first need to instatiate the dataparallel model \n",
    "if you start fresh then first model.load_weights and then make it parallel\n",
    "'''\n",
    "try:\n",
    "    PATH = '../pretrained.pth'\n",
    "    weights = torch.load(PATH)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we https://pytorch.org/docs/stable/data.html#torch.utils.data.Datasetare on a CUDA machine, this should print a CUDA device:\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 9:\n",
    "        print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(net)\n",
    "        model.to(device)\n",
    "        model.load_state_dict(weights)\n",
    "    else:\n",
    "        model=net\n",
    "        model.to(device)\n",
    "        model.load_state_dict(weights)\n",
    "        \n",
    "except FileNotFoundError: \n",
    "    net.load_weights(\"../yolov3.weights\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(net)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        model=net\n",
    "        \n",
    "        \n",
    "transformed_dataset=Coco(partition='train',\n",
    "                                           transform=transforms.Compose([\n",
    "                                            ResizeToTensor(inp_dim)\n",
    "                                           ]))\n",
    "\n",
    "\n",
    "writer = SummaryWriter('../results/test')\n",
    "dataset_len=(len(transformed_dataset))\n",
    "print('Length of dataset is '+ str(dataset_len)+'\\n')\n",
    "batch_size=8\n",
    "\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                        shuffle=True,collate_fn=helper.my_collate, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, weight_decay=0.0005, momentum=0.9)\n",
    "\n",
    "lambda1 = lambda epoch: 0.95**epoch\n",
    "scheduler=optim.lr_scheduler.LambdaLR(optimizer, lambda1, last_epoch=-1)\n",
    "epochs=50\n",
    "total_loss=0\n",
    "write=0\n",
    "misses=0\n",
    "break_flag=0\n",
    "avg_iou=0\n",
    "for e in range(epochs):\n",
    "    prg_counter=0\n",
    "    train_counter=0\n",
    "    total_loss=0\n",
    "    avg_iou=0\n",
    "    avg_infs=0\n",
    "    avg_conf=0\n",
    "    avg_no_conf=0\n",
    "    avg_pos=0\n",
    "    avg_neg=0\n",
    "    print(\"\\n epoch \"+str(e))\n",
    "    misses=0\n",
    "    for images,targets,img_names in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        targets,anchors,offset,strd,mask=helper.collapse_boxes(targets,pw_ph,cx_cy,stride)\n",
    "        images=images.cuda()\n",
    "        raw_pred = model(images, torch.cuda.is_available())\n",
    "        raw_pred=helper.expand_predictions(raw_pred,mask)\n",
    "        true_pred=util.transform(raw_pred.clone(),anchors,offset,strd)\n",
    "        targets=targets.unsqueeze(-3)\n",
    "        targets=targets.cuda()\n",
    "        iou_mask,noobj_mask=util.get_responsible_masks(true_pred,targets,offset,strd,mask,inp_dim)\n",
    "        \n",
    "        iou1=torch.diag(util.bbox_iou(util.get_abs_coord(true_pred[iou_mask.T,:].unsqueeze(-3)),util.transpose_target(get_abs_coord(targets)*inp_dim)))\n",
    "        iou=iou1.mean().item()\n",
    "        noobj_box=raw_pred[:,:,4:5].clone()\n",
    "        noobj_box=helper.uncollapse(noobj_box,mask)\n",
    "        noobj_mask=helper.uncollapse(noobj_mask.T.unsqueeze(-1),mask)\n",
    "        \n",
    "        \n",
    "        conf=raw_pred[iou_mask.T,:][:,4:5].mean().item()\n",
    "        class_mask=targets[:,:,5:].type(torch.BoolTensor).squeeze(0)\n",
    "        if(iou_mask.sum()==class_mask.shape[0]):\n",
    "            pos_class=raw_pred[iou_mask.T,:][:,5:][class_mask].mean().item()\n",
    "            neg_class=raw_pred[iou_mask.T,:][:,5:][~class_mask].mean().item()\n",
    "        else:\n",
    "            pos_class=0\n",
    "            neg_class=0\n",
    "        noobj_box=noobj_box[noobj_mask]\n",
    "        no_obj_conf=noobj_box.mean().item()\n",
    "        \n",
    "        raw_pred=raw_pred[iou_mask.T,:]\n",
    "        anchors=anchors[iou_mask.T,:]\n",
    "        offset=offset[iou_mask.T,:]\n",
    "        strd=strd[iou_mask.T,:]\n",
    "        if(strd.shape[0]==sum(mask)):#this means that iou_mask failed and was all true, because max of zeros is true for all lenght of mask strd\n",
    "            targets[:,:,0:4]=targets[:,:,0:4]*inp_dim\n",
    "            targets=targets.squeeze(0)\n",
    "            targets[:,0:4]=util.transform_groundtruth(targets,anchors,offset,strd)\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                loss=util.yolo_loss(raw_pred,targets,noobj_box,mask)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            avg_conf=avg_conf+conf\n",
    "            avg_no_conf=avg_no_conf+no_obj_conf\n",
    "            avg_pos=avg_pos+pos_class\n",
    "            avg_neg=avg_neg+neg_class\n",
    "            total_loss=total_loss+loss.item()\n",
    "            avg_iou=avg_iou+iou\n",
    "            sys.stdout.write('\\rPgr:'+str(prg_counter/dataset_len*100*batch_size)+'%' ' L:'+ str(loss.item()))\n",
    "            sys.stdout.write(' IoU:' +str(iou)+' pob:'+str(conf)+ ' nob:'+str(no_obj_conf))\n",
    "            sys.stdout.write(' PCls:' +str(pos_class)+' ncls:'+str(neg_class))\n",
    "            sys.stdout.flush()\n",
    "            prg_counter=prg_counter+1\n",
    "            train_counter=train_counter+1\n",
    "        else:\n",
    "            print('missed')\n",
    "            print(strd.shape[0])\n",
    "            prg_counter=prg_counter+1\n",
    "        iou1=iou1.tolist()\n",
    "        iou_per_class= [0] * 80\n",
    "        averager_per_class=[1] * 80\n",
    "        for i,el in enumerate(iou1):\n",
    "            name=targets[i,5:].max(0)[1].cpu().detach().numpy()\n",
    "            iou_per_class[name]=iou_per_class[name]+el\n",
    "            averager_per_class[name]=averager_per_class[name]+1\n",
    "        for i in range(80):\n",
    "            if (iou_per_class[i]!=0):\n",
    "                if i<40:\n",
    "                    writer.add_scalar('Iou0/'+str(i), iou_per_class[i]/averager_per_class[i], train_counter)\n",
    "                else:\n",
    "                    writer.add_scalar('Iou1/'+str(i), iou_per_class[i]/averager_per_class[i], train_counter)\n",
    "        writer.add_scalar('AvLoss/train', total_loss/train_counter, train_counter)\n",
    "\n",
    "        \n",
    "        writer.add_scalar('AvIoU/train', avg_iou/train_counter, train_counter)\n",
    "\n",
    "        writer.add_scalar('AvPConf/train', avg_conf/train_counter, train_counter)\n",
    "        \n",
    "        writer.add_scalar('AvNConf/train', avg_no_conf/train_counter, train_counter)\n",
    "        \n",
    "        writer.add_scalar('AvClass/train', avg_pos/train_counter, train_counter)\n",
    "        \n",
    "        writer.add_scalar('AvNClass/train', avg_neg/train_counter, train_counter)\n",
    "        \n",
    "        del loss, raw_pred, targets, true_pred, images,iou,noobj_box,conf,iou1\n",
    "        torch.cuda.empty_cache()\n",
    "    if misses>0:\n",
    "        break\n",
    "#     torch.save(model.state_dict(), PATH)\n",
    "#     writer.add_scalar('Loss/train', total_loss/train_counter, e)\n",
    "#     writer.add_scalar('AIoU/train', avg_iou/train_counter, e)\n",
    "#     writer.add_scalar('PConf/train', avg_conf/train_counter, e)\n",
    "#     writer.add_scalar('NConf/train', avg_no_conf/train_counter, e)\n",
    "#     writer.add_scalar('PClass/train', avg_pos/train_counter, e)\n",
    "#     writer.add_scalar('NClass/train', avg_neg/train_counter, e)\n",
    "    print('\\ntotal number of misses is ' + str(misses))\n",
    "    print('\\n total average loss is '+str(total_loss/train_counter))\n",
    "    print('\\n total average iou is '+str(avg_iou/train_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.6239,  4.3937],\n",
      "        [ 8.4988, 10.0201],\n",
      "        [ 1.0580,  2.4166],\n",
      "        [ 0.9973,  2.6048],\n",
      "        [ 3.2327,  3.0643],\n",
      "        [ 0.5482,  0.5809],\n",
      "        [ 3.2654,  2.5082],\n",
      "        [ 1.6980,  3.4301],\n",
      "        [ 0.3344,  0.3726],\n",
      "        [ 1.7152,  2.2961],\n",
      "        [ 0.6986,  1.4767],\n",
      "        [ 0.9299,  0.6364],\n",
      "        [ 2.5822,  3.2134],\n",
      "        [ 1.0680,  1.3578],\n",
      "        [ 1.3028,  2.9742],\n",
      "        [ 0.4619,  1.3526],\n",
      "        [ 2.1216,  1.8432],\n",
      "        [ 0.6065,  1.3174],\n",
      "        [ 0.3986,  1.9597],\n",
      "        [ 1.1533,  1.6773],\n",
      "        [ 0.6278,  1.6535],\n",
      "        [ 1.8403,  4.1132],\n",
      "        [ 0.7894,  1.0318],\n",
      "        [ 5.3638,  1.1324],\n",
      "        [ 2.6491,  1.1525],\n",
      "        [ 0.9253,  0.7233],\n",
      "        [ 0.7345,  1.2702],\n",
      "        [ 1.1153,  0.3788],\n",
      "        [ 0.7916,  2.8667],\n",
      "        [ 2.4124,  1.9173],\n",
      "        [ 0.6108,  1.2117],\n",
      "        [ 0.7077,  2.5864],\n",
      "        [ 7.2755,  1.3777],\n",
      "        [ 1.0825,  1.0979],\n",
      "        [ 0.5145,  0.8254],\n",
      "        [ 0.9801,  0.3614],\n",
      "        [ 1.1240,  1.2858],\n",
      "        [ 1.5987,  1.6328],\n",
      "        [ 1.2858,  1.1851]], device='cuda:0')\n",
      "tensor([0.5217, 0.5029, 0.5966, 0.5951, 0.5252, 0.9585, 0.5305, 0.5428, 0.9997,\n",
      "        0.5631, 0.7250, 0.8442, 0.5301, 0.6659, 0.5642, 0.8321, 0.5636, 0.7776,\n",
      "        0.7825, 0.6264, 0.7238, 0.5330, 0.7734, 0.5411, 0.5812, 0.8167, 0.7449,\n",
      "        0.9143, 0.6084, 0.5538, 0.7943, 0.6333, 0.5249, 0.6988, 0.9133, 0.9439,\n",
      "        0.6664, 0.5946, 0.6584], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.exp(targets[:,2:4]))\n",
    "wh=torch.exp(targets[:,2:4])\n",
    "area=wh[:,0]*wh[:,1]\n",
    "weights=torch.sigmoid(1/area).cuda()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  4.,  1.,  0.,  0.,  1.,\n",
      "         0.,  0.,  1.,  1.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,\n",
      "         2.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
      "         0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
      "[2, 4, 6, 8, 9, 2, 2, 6]\n",
      "39\n",
      "tensor([ 0, 38,  0,  0, 34,  0, 54, 54, 54, 54, 54, 63,  0,  0, 37, 37, 37, 37,\n",
      "         0,  0, 72, 56,  0, 44, 45, 46, 46, 60, 69, 22, 22,  0, 36, 57,  0, 65,\n",
      "        41, 73, 56], device='cuda:0')\n",
      "tensor([1.2657, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 2.9704, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.6636, 0.0000,\n",
      "        3.6636, 2.2773, 3.6636, 0.0000, 0.0000, 3.6636, 0.0000, 0.0000, 3.6636,\n",
      "        3.6636, 2.9704, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        2.0541, 0.0000, 2.9704, 3.6636, 0.0000, 0.0000, 3.6636, 0.0000, 0.0000,\n",
      "        3.6636, 0.0000, 3.6636, 0.0000, 0.0000, 0.0000, 3.6636, 0.0000, 0.0000,\n",
      "        3.6636, 3.6636, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0')\n",
      "tensor([1.2657, 3.6636, 1.2657, 1.2657, 3.6636, 1.2657, 2.0541, 2.0541, 2.0541,\n",
      "        2.0541, 2.0541, 3.6636, 1.2657, 1.2657, 2.2773, 2.2773, 2.2773, 2.2773,\n",
      "        1.2657, 1.2657, 3.6636, 2.9704, 1.2657, 3.6636, 3.6636, 2.9704, 2.9704,\n",
      "        3.6636, 3.6636, 2.9704, 2.9704, 1.2657, 3.6636, 3.6636, 1.2657, 3.6636,\n",
      "        3.6636, 3.6636, 2.9704], device='cuda:0')\n",
      "tensor([0.6328, 1.8318, 0.3164, 0.3164, 0.9159, 0.3164, 0.3424, 0.3424, 0.3424,\n",
      "        0.3424, 0.3424, 0.6106, 0.1582, 0.1582, 0.2847, 0.2847, 0.2847, 0.2847,\n",
      "        0.1582, 0.1582, 0.4071, 0.3300, 0.1406, 0.4071, 0.4071, 0.3300, 0.3300,\n",
      "        0.4071, 0.4071, 1.4852, 1.4852, 0.6328, 1.8318, 0.6106, 0.2109, 0.6106,\n",
      "        0.6106, 0.6106, 0.4951], device='cuda:0')\n",
      "tensor([0.0256, 0.0848, 0.0186, 0.0186, 0.0339, 0.0186, 0.0191, 0.0191, 0.0191,\n",
      "        0.0191, 0.0191, 0.0250, 0.0159, 0.0159, 0.0180, 0.0180, 0.0180, 0.0180,\n",
      "        0.0159, 0.0159, 0.0204, 0.0189, 0.0156, 0.0204, 0.0204, 0.0189, 0.0189,\n",
      "        0.0204, 0.0204, 0.0599, 0.0599, 0.0256, 0.0848, 0.0250, 0.0168, 0.0250,\n",
      "        0.0250, 0.0250, 0.0223], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "print(targets[:,5:].sum(axis=0))\n",
    "print(mask)\n",
    "print(sum(mask))\n",
    "classes=targets[:,5:].max(1)[1]\n",
    "print(classes)\n",
    "idf=torch.log(sum(mask)/targets[:,5:].sum(axis=0))\n",
    "idf[idf== float('inf')] = 0\n",
    "print(idf)\n",
    "print(idf[classes])\n",
    "\n",
    "tf=[1/mask[i] for i in range(len(mask)) for j in range(mask[i])]\n",
    "tf=torch.tensor([1/mask[i] for i in range(len(mask)) for j in range(mask[i])]).cuda()\n",
    "tfidf=tf*idf[classes]\n",
    "print(tfidf)\n",
    "\n",
    "print(torch.softmax(tfidf,dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets2,anchors,offset,strd,mask=helper.collapse_boxes(targets,pw_ph,cx_cy,stride)\n",
    "transformed_output=true_pred\n",
    "targets=targets\n",
    "offset=offset\n",
    "strd=strd\n",
    "mask=mask\n",
    "inp_dim\n",
    "'''\n",
    "this function takes the transformed_output and\n",
    "the target box in respect to the resized image size\n",
    "and returns a mask which can be applied to select the \n",
    "best raw input,anchors and cx_cy_offset\n",
    "and the noobj_mask for the negatives\n",
    "targets is a list\n",
    "'''\n",
    "#first transpose the centered normalised target coords\n",
    "centered_target=transpose_target(targets)[:,:,0:2]\n",
    "#multiply by inp_dim then devide by stride to get the relative grid size coordinates, floor the result to get the corresponding cell\n",
    "centered_target=torch.floor(centered_target*inp_dim/strd)\n",
    "#create a mask to find where the gt falls into which gridcell in the grid coordinate system\n",
    "fall_into_mask=centered_target==offset\n",
    "fall_into_mask=fall_into_mask[:,:,0]&fall_into_mask[:,:,1]\n",
    "#     fall_into_mask= ~fall_into_mask\n",
    "#create a copy of the transformed output\n",
    "best_bboxes=transformed_output.clone()\n",
    "#apply reverse mask to copy in order to zero all other bbox locations\n",
    "best_bboxes[~fall_into_mask]=0   \n",
    "#transform the copy to xmin,xmax,ymin,ymax\n",
    "best_responsible_coord=get_abs_coord(best_bboxes)\n",
    "targets=transpose_target(get_abs_coord(targets))*inp_dim\n",
    "#calculate best iou and mask\n",
    "responsible_iou=bbox_iou(best_responsible_coord,targets,True)\n",
    "\n",
    "responsible_iou[responsible_iou.ne(responsible_iou)] = 0\n",
    "responsible_mask=responsible_iou.max(dim=0)[0] == responsible_iou\n",
    "\n",
    "print(responsible_mask.shape)\n",
    "\n",
    "abs_coord=get_abs_coord(transformed_output)\n",
    "iou=bbox_iou(abs_coord,targets,True)\n",
    "iou[iou.ne(iou)] = 0\n",
    "ignore_mask=0.5<=iou\n",
    "inverted_mask=iou.max(dim=0)[0] != iou\n",
    "noobj_mask=~same_picture_mask(responsible_mask.clone()|ignore_mask,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(responsible_mask)\n",
    "if(responsible_mask.sum()>sum(mask)):\n",
    "    print('jello')\n",
    "    responsible_mask1=correct_iou_mask(responsible_mask,fall_into_mask)\n",
    "print(responsible_mask1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.randint(0, 9, (1,)) == torch.arange(9)\n",
    "a=a.repeat(5)\n",
    "a=a.repeat(2,1)\n",
    "print(a)\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_mask=(responsible_mask.sum(axis=0)==responsible_mask.sum(axis=0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsible_mask[:,le_mask]=(responsible_mask[:,le_mask]&fall_into_mask[le_mask,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(le_mask)\n",
    "print(fall_into_mask[le_mask,:].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([1,2,3,4,5,6,6,6])\n",
    "indices=(((a==a.max())==True).nonzero())\n",
    "for ind in indices:\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Using  2 GPUs!\n",
      "testing with ../pretrained.pth\n",
      "\n",
      "Length of dataset is 5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/site-packages/torch/nn/functional.py:2503: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    }
   ],
   "source": [
    "from darknet import *\n",
    "import darknet as dn\n",
    "import util as util\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import timeit\n",
    "from dataset import *\n",
    "import torchvision.ops.boxes as nms_box\n",
    "import helper as helper\n",
    "from utils import *\n",
    "from BoundingBox import BoundingBox\n",
    "from BoundingBoxes import BoundingBoxes\n",
    "\n",
    "\n",
    "net = Darknet(\"../cfg/yolov3.cfg\")\n",
    "inp_dim=net.inp_dim\n",
    "pw_ph=net.pw_ph\n",
    "cx_cy=net.cx_cy\n",
    "stride=net.stride\n",
    "\n",
    "\n",
    "'''\n",
    "when loading weights from dataparallel model then, you first need to instatiate the dataparallel model \n",
    "if you start fresh then first model.load_weights and then make it parallel\n",
    "'''\n",
    "try:\n",
    "    PATH = '../pretrained.pth'\n",
    "    weights = torch.load(PATH)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(net)\n",
    "    else:\n",
    "        model=net\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "except FileNotFoundError: \n",
    "    net.load_weights(\"../yolov3.weights\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "      print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "      model = nn.DataParallel(net)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('testing with '+ PATH +'\\n')\n",
    "transformed_dataset=Coco(partition='val',\n",
    "                                           transform=transforms.Compose([\n",
    "                                            ResizeToTensor(inp_dim)\n",
    "                                           ]))\n",
    "\n",
    "\n",
    "\n",
    "dataset_len=(len(transformed_dataset))\n",
    "print('Length of dataset is '+ str(dataset_len)+'\\n')\n",
    "batch_size=8\n",
    "\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                        shuffle=True,collate_fn=helper.my_collate, num_workers=2)\n",
    "\n",
    "true_pos=0\n",
    "false_pos=0\n",
    "counter=0\n",
    "iou_threshold=0.5\n",
    "confidence=0.01\n",
    "recall_counter=0\n",
    "\n",
    "for images,targets,img_name in dataloader:\n",
    "    inp=images.cuda()\n",
    "    raw_pred = model(inp, torch.cuda.is_available())\n",
    "    targets,anchors,offset,strd,mask=helper.collapse_boxes(targets,pw_ph,cx_cy,stride)\n",
    "    raw_pred=raw_pred.to(device='cuda')\n",
    "    true_pred=util.transform(raw_pred.clone(),pw_ph,cx_cy,stride)\n",
    "    \n",
    "    classes=true_pred[:,:,5:].max(2)[0]\n",
    "    objectness=true_pred[:,:,4]\n",
    "    \n",
    "    sorted_pred=torch.sort(objectness,descending=True)\n",
    "    pred_mask=sorted_pred[0]>confidence\n",
    "    indices=[(sorted_pred[1][e,:][pred_mask[e,:]]) for e in range(pred_mask.shape[0])]\n",
    "    pred_final=[true_pred[i,indices[i],:] for i in range(len(indices))]\n",
    "    \n",
    "    pred_final_coord=[util.get_abs_coord(pred_final[i].unsqueeze(-2)) for i in range(len(pred_final))]\n",
    "    \n",
    "    indices=[nms_box.nms(pred_final_coord[i][0],pred_final[i][:,4],iou_threshold) for i in range(len(pred_final))]\n",
    "\n",
    "    pred_final=[pred_final[i][indices[i],:] for i in range(len(pred_final))]\n",
    "#     pred_final[:,0:4]=pred_final[:,0:4]/inp_dim\n",
    "    helper.write_pred(img_name,pred_final,inp_dim)\n",
    "    \n",
    "\n",
    "    \n",
    "# Read txt files containing bounding boxes (ground truth and detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darknet import *\n",
    "import darknet as dn\n",
    "import util as util\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import timeit\n",
    "from dataset import *\n",
    "import torchvision.ops.boxes as nms_box\n",
    "import helper as helper\n",
    "from utils import *\n",
    "from BoundingBox import BoundingBox\n",
    "from BoundingBoxes import BoundingBoxes\n",
    "boundingboxes = helper.getBoundingBoxes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COCO map is: 0.6697134191784458\n",
      "\n",
      "COCO map is: 1.3352560992330769\n",
      "\n",
      "COCO map is: 1.9964744098491396\n",
      "\n",
      "COCO map is: 2.653250136699156\n",
      "\n",
      "COCO map is: 3.3056622952264365\n",
      "\n",
      "COCO map is: 3.951772607448092\n",
      "\n",
      "COCO map is: 4.589426216164657\n",
      "\n",
      "COCO map is: 5.216961243164041\n",
      "\n",
      "COCO map is: 5.832269768213639\n",
      "\n",
      "COCO map is: 6.4300187989023545\n",
      "\n",
      "COCO map is: 7.00296698428485\n",
      "\n",
      "COCO map is: 7.540756813394808\n",
      "\n",
      "COCO map is: 8.031861349906901\n",
      "\n",
      "COCO map is: 8.464277006412278\n",
      "\n",
      "COCO map is: 8.817369154686132\n",
      "\n",
      "COCO map is: 9.074098803584556\n",
      "\n",
      "COCO map is: 9.21308321890343\n",
      "\n",
      "COCO map is: 9.254349068108349\n"
     ]
    }
   ],
   "source": [
    "from Evaluator import *\n",
    "from utils import *\n",
    "from BoundingBox import BoundingBox\n",
    "from BoundingBoxes import BoundingBoxes\n",
    "\n",
    "\n",
    "boundingboxes = helper.getBoundingBoxes()\n",
    "\n",
    "evaluator = Evaluator()\n",
    "\n",
    "iou=0.05\n",
    "MMap=0\n",
    "while iou<0.95:\n",
    "    metricsPerClass = evaluator.GetPascalVOCMetrics(boundingboxes, IOUThreshold=iou)\n",
    "#     print(\"Average precision values per class:\\n\")\n",
    "    # Loop through classes to obtain their metrics\n",
    "    mAP=0\n",
    "    counter=0\n",
    "    for mc in metricsPerClass:\n",
    "        # Get metric values per each class\n",
    "        c = mc['class']\n",
    "        precision = mc['precision']\n",
    "        recall = mc['recall']\n",
    "        average_precision = mc['AP']\n",
    "        ipre = mc['interpolated precision']\n",
    "        irec = mc['interpolated recall']\n",
    "        # Print AP per class\n",
    "        mAP=average_precision+mAP\n",
    "#         print('%s: %f' % (c, average_precision))\n",
    "    MMap=MMap+mAP/80\n",
    "    iou=iou+0.05\n",
    "    print('\\nCOCO map is:',MMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.044080\n",
      "1: 0.018950\n",
      "10: 0.083119\n",
      "11: 0.099939\n",
      "12: 0.079374\n",
      "13: 0.029754\n",
      "14: 0.019329\n",
      "15: 0.116021\n",
      "16: 0.093195\n",
      "17: 0.063567\n",
      "18: 0.051430\n",
      "19: 0.067876\n",
      "2: 0.027003\n",
      "20: 0.087345\n",
      "21: 0.230473\n",
      "22: 0.079109\n",
      "23: 0.091553\n",
      "24: 0.000512\n",
      "25: 0.019484\n",
      "26: 0.002928\n",
      "27: 0.014346\n",
      "28: 0.021513\n",
      "29: 0.033620\n",
      "3: 0.014385\n",
      "30: 0.005631\n",
      "31: 0.000630\n",
      "32: 0.024367\n",
      "33: 0.023056\n",
      "34: 0.003730\n",
      "35: 0.033894\n",
      "36: 0.008477\n",
      "37: 0.012159\n",
      "38: 0.024032\n",
      "39: 0.014296\n",
      "4: 0.112498\n",
      "40: 0.014734\n",
      "41: 0.046908\n",
      "42: 0.017874\n",
      "43: 0.010425\n",
      "44: 0.007936\n",
      "45: 0.041454\n",
      "46: 0.015823\n",
      "47: 0.018162\n",
      "48: 0.062845\n",
      "49: 0.036478\n",
      "5: 0.195222\n",
      "50: 0.004256\n",
      "51: 0.002300\n",
      "52: 0.031162\n",
      "53: 0.070215\n",
      "54: 0.024261\n",
      "55: 0.027452\n",
      "56: 0.009959\n",
      "57: 0.103516\n",
      "58: 0.007340\n",
      "59: 0.081101\n",
      "6: 0.124600\n",
      "60: 0.068044\n",
      "61: 0.088627\n",
      "62: 0.061185\n",
      "63: 0.096142\n",
      "64: 0.008939\n",
      "65: 0.004051\n",
      "66: 0.057793\n",
      "67: 0.018215\n",
      "68: 0.050436\n",
      "69: 0.015009\n",
      "7: 0.030365\n",
      "70: 0.000000\n",
      "71: 0.021644\n",
      "72: 0.073670\n",
      "73: 0.001596\n",
      "74: 0.012959\n",
      "75: 0.027426\n",
      "76: 0.008547\n",
      "77: 0.039591\n",
      "78: 0.000000\n",
      "79: 0.001276\n",
      "8: 0.003643\n",
      "9: 0.006412\n"
     ]
    }
   ],
   "source": [
    "for mc in metricsPerClass:\n",
    "    c = mc['class']\n",
    "    average_precision = mc['AP']\n",
    "    print('%s: %f' % (c, average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48707100358464994\n"
     ]
    }
   ],
   "source": [
    "print(MMap/19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9: 0.000231\n"
     ]
    }
   ],
   "source": [
    "print('%s: %f' % (c, average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f162f914160>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluator import *\n",
    "evaluator = Evaluator()\n",
    "metricsPerClass = evaluator.GetPascalVOCMetrics(boundingboxes, IOUThreshold=0.5)\n",
    "print(\"Average precision values per class:\\n\")\n",
    "# Loop through classes to obtain their metrics\n",
    "mAP=0\n",
    "counter=0\n",
    "for mc in metricsPerClass:\n",
    "    # Get metric values per each class\n",
    "    c = mc['class']\n",
    "    precision = mc['precision']\n",
    "    recall = mc['recall']\n",
    "    average_precision = mc['AP']\n",
    "    ipre = mc['interpolated precision']\n",
    "    irec = mc['interpolated recall']\n",
    "    # Print AP per class\n",
    "    mAP=average_precision+mAP\n",
    "    print('%s: %f' % (c, average_precision))\n",
    "\n",
    "print('map is:',mAP/80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.07290217209136615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "coord=pred_final[:,:4].cpu().detach().numpy()\n",
    "conf=pred_final[:,4:5].cpu().detach().numpy()\n",
    "mat=np.hstack((conf,coord))\n",
    "\n",
    "classes=pred_final[:,5:].max(1)[1].cpu().detach().numpy()\n",
    "classes=np.array([classes]).T\n",
    "\n",
    "mat=np.hstack((classes,mat))\n",
    "mat=np.array(mat)\n",
    "\n",
    "df=pd.DataFrame(mat,index=None,columns=None)\n",
    "df[0]=df[0].apply(lambda x: int(x))\n",
    "\n",
    "df.to_csv('test.txt',sep=' ',header=False,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('test.txt',mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([1,2,2,3,4,2,3,1,4])\n",
    "print(a.max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=(inp).squeeze(0)\n",
    "image=np.array(image.cpu())\n",
    "print(image.shape)\n",
    "image =  image[:,:,::-1].transpose((1,2,0))\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "all_files = glob.glob('plots/' + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in sorted(all_files):\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=1, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# gca stands for 'get current axis'\n",
    "\n",
    "title_list=['AIoU_train','Loss_train','NClass_train','NConf_train','PClass','PConf']\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "fig.suptitle('KL for xy loss')\n",
    "fig.subplots_adjust(hspace=0.3, wspace=-.6)\n",
    "colors = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c']\n",
    "i=2\n",
    "k=0\n",
    "while i <18:\n",
    "    ax = fig.add_subplot(2, 9, i)\n",
    "    frame.plot(x =1 , y = i,ax=ax,legend=False)\n",
    "    ax.set_title(title_list[k])\n",
    "    i=i+3\n",
    "    k=k+1\n",
    "plt.savefig('original.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
