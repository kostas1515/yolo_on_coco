{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import cv2\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='labels/coco2017labels.zip'\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "#     bbox=zip.read('coco/labels/train2017/000000371735.txt')\n",
    "#     box=(bbox.decode(\"utf-8\"))\n",
    "#     box=pd.DataFrame([x.split() for x in box.rstrip('\\n').split('\\n')],columns=['class','xc','yc','w','h'])\n",
    "#     print(box)\n",
    "    count = len(zip.infolist())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name='labels/coco2017labels.zip'\n",
    "zip_file = ZipFile(file_name)\n",
    "\n",
    "dfs = {text_file.filename: pd.read_csv(zip_file.open(text_file.filename))\n",
    "       for text_file in zip_file.infolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a=torch.tensor([[[1,2,2,4,0,0,0,1],[2,3,5,3,0,0,1,0]]])\n",
    "print(a.shape)\n",
    "b=((a[:,:,4:]==1).nonzero())\n",
    "b=b[:,-1].unsqueeze(0).unsqueeze(-1)\n",
    "print(b.shape)\n",
    "new=a[:,:,:4]\n",
    "print(new.shape)\n",
    "new=torch.cat((new.T,b.T)).T\n",
    "print(new)\n",
    "sorted_pred=torch.sort(new[0,:,4],descending=False)\n",
    "print(sorted_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = box.values.astype(np.float32)\n",
    "b=torch.tensor(b)\n",
    "labels = b.T[0].reshape(b.shape[0], 1)\n",
    "one_hot_target = (labels == torch.arange(80).reshape(1, 80)).float()\n",
    "conf=torch.ones([b.shape[0],1])\n",
    "boxes=torch.cat((b.T[1:],conf.T,one_hot_target.T)).T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='images/train2017.zip'\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    image=zip.read('train2017/000000408542.jpg')\n",
    "    img = cv2.imdecode(np.frombuffer(image, np.uint8),1)\n",
    "    im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    io.imshow(im_rgb)\n",
    "    io.imsave('fig.png',im_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('pointers/train2017.txt',names=['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['box']=df['filename'].apply(lambda x: 'coco/labels/'+x.split('.')[0]+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Using  2 GPUs!\n",
      "Length of dataset is 118287\n",
      "\n",
      "\n",
      " epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/site-packages/torch/nn/functional.py:2503: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pgr:0.013526423021971984% L:2.3660693168640137 IoU:0.7562940716743469 pob:0.4892883896827698 nob:0.0003056609129998833 PCls:0.8889921307563782 ncls:0.0014051623875275254"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-82e9a150a2b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miou\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoobj_box\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miou1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmisses\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/faster_rcnn/lib/python3.8/site-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dataset import *\n",
    "import timeit \n",
    "import cv2\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from darknet import *\n",
    "import darknet as dn\n",
    "import util as util\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import timeit\n",
    "import torch.autograd\n",
    "import helper as helper\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "net = Darknet(\"../cfg/yolov3.cfg\")\n",
    "inp_dim=net.inp_dim\n",
    "pw_ph=net.pw_ph.to(device='cuda')\n",
    "cx_cy=net.cx_cy.to(device='cuda')\n",
    "stride=net.stride.to(device='cuda')\n",
    "\n",
    "hyperparameters={'lr':0.0001,\n",
    "                 'batch_size':16,\n",
    "                 'weight_decay':0.001,\n",
    "                 'momentum':0.9,\n",
    "                 'optimizer':'sgd',\n",
    "                 'alpha':0.5,\n",
    "                 'gamma':2,\n",
    "                 'lcoord':5,\n",
    "                 'lno_obj':1,\n",
    "                 'iou_type':(0,0,0),#(GIoU,DIoU,CIoU) default is 0,0,0 for iou\n",
    "                 'iou_ignore_thresh':0.5,\n",
    "                 'tfidf':True,\n",
    "                 'workers':2,\n",
    "                 'gpus':2,\n",
    "                 'path':'pretrained',\n",
    "                 'reduction':'sum'}\n",
    "\n",
    "\n",
    "'''\n",
    "when loading weights from dataparallel model then, you first need to instatiate the dataparallel model \n",
    "if you start fresh then first model.load_weights and then make it parallel\n",
    "'''\n",
    "try:\n",
    "    PATH = '../'+hyperparameters['path']\n",
    "    weights = torch.load(PATH)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we https://pytorch.org/docs/stable/data.html#torch.utils.data.Datasetare on a CUDA machine, this should print a CUDA device:\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    if (torch.cuda.device_count() > 1)&(hyperparameters['gpus']>1):\n",
    "        print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(net)\n",
    "        model.to(device)\n",
    "        model.load_state_dict(weights)\n",
    "    else:\n",
    "        model=net\n",
    "        model.to(device)\n",
    "        model.load_state_dict(weights)\n",
    "        \n",
    "except FileNotFoundError: \n",
    "    net.load_weights(\"../yolov3.weights\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "    if (torch.cuda.device_count() > 1)&(hyperparameters['gpus']>1):\n",
    "        print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(net)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        model=net\n",
    "        \n",
    "\n",
    "transformed_dataset=Coco(partition='train',\n",
    "                                           transform=transforms.Compose([\n",
    "                                            ResizeToTensor(inp_dim),\n",
    "                                           ]))\n",
    "\n",
    "\n",
    "writer = SummaryWriter('../results/'+hyperparameters['path'])\n",
    "dataset_len=(len(transformed_dataset))\n",
    "print('Length of dataset is '+ str(dataset_len)+'\\n')\n",
    "batch_size=hyperparameters['batch_size']\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                        shuffle=True,collate_fn=helper.my_collate, num_workers=hyperparameters['workers'])\n",
    "\n",
    "\n",
    "if hyperparameters['optimizer']=='sgd':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=hyperparameters['lr'], weight_decay=hyperparameters['weight_decay'], momentum=hyperparameters['momentum'])\n",
    "elif hyperparameters['otimizer']=='adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hyperparameters['lr'], weight_decay=hyperparameters['weight_decay'])\n",
    "\n",
    "lambda1 = lambda epoch: 0.95**epoch\n",
    "scheduler=optim.lr_scheduler.LambdaLR(optimizer, lambda1, last_epoch=-1)\n",
    "\n",
    "epochs=50\n",
    "total_loss=0\n",
    "write=0\n",
    "misses=0\n",
    "avg_iou=0\n",
    "for e in range(epochs):\n",
    "    prg_counter=0\n",
    "    train_counter=0\n",
    "    total_loss=0\n",
    "    avg_iou=0\n",
    "    avg_infs=0\n",
    "    avg_conf=0\n",
    "    avg_no_conf=0\n",
    "    avg_pos=0\n",
    "    avg_neg=0\n",
    "    print(\"\\n epoch \"+str(e))\n",
    "    misses=0\n",
    "    for images,targets,img_names in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        targets,anchors,offset,strd,mask=helper.collapse_boxes(targets,pw_ph,cx_cy,stride)\n",
    "        images=images.cuda()\n",
    "        raw_pred = model(images, torch.cuda.is_available())\n",
    "        raw_pred=helper.expand_predictions(raw_pred,mask)\n",
    "        true_pred=util.transform(raw_pred.clone(),anchors,offset,strd)\n",
    "        targets=targets.unsqueeze(-3)\n",
    "        targets=targets.cuda()\n",
    "        iou_mask,noobj_mask=util.get_responsible_masks(true_pred,targets,offset,strd,mask,inp_dim,hyperparameters)\n",
    "        \n",
    "        iou1=torch.diag(util.bbox_iou(util.get_abs_coord(true_pred[iou_mask.T,:].unsqueeze(-3)),util.transpose_target(get_abs_coord(targets)*inp_dim),hyperparameters['iou_type']))\n",
    "        iou=iou1.mean().item()\n",
    "        noobj_box=raw_pred[:,:,4:5].clone()\n",
    "        noobj_box=helper.uncollapse(noobj_box,mask)\n",
    "        noobj_mask=helper.uncollapse(noobj_mask.T.unsqueeze(-1),mask)\n",
    "        \n",
    "        \n",
    "        conf=raw_pred[iou_mask.T,:][:,4:5].mean().item()\n",
    "        class_mask=targets[:,:,5:].type(torch.BoolTensor).squeeze(0)\n",
    "        if(iou_mask.sum()==class_mask.shape[0]):\n",
    "            pos_class=raw_pred[iou_mask.T,:][:,5:][class_mask].mean().item()\n",
    "            neg_class=raw_pred[iou_mask.T,:][:,5:][~class_mask].mean().item()\n",
    "        else:\n",
    "            pos_class=0\n",
    "            neg_class=0\n",
    "            \n",
    "        noobj_box=noobj_box[noobj_mask]\n",
    "        no_obj_conf=noobj_box.mean().item()\n",
    "        \n",
    "        raw_pred=raw_pred[iou_mask.T,:]\n",
    "        anchors=anchors[iou_mask.T,:]\n",
    "        offset=offset[iou_mask.T,:]\n",
    "        strd=strd[iou_mask.T,:]\n",
    "        \n",
    "        if(strd.shape[0]==sum(mask)):#this means that iou_mask failed and was all true, because max of zeros is true for all lenght of mask strd\n",
    "            targets[:,:,0:4]=targets[:,:,0:4]*inp_dim\n",
    "            targets=targets.squeeze(0)\n",
    "            targets[:,0:4]=util.transform_groundtruth(targets,anchors,offset,strd)\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                loss=util.yolo_loss(raw_pred,targets,noobj_box,mask,hyperparameters)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            avg_conf=avg_conf+conf\n",
    "            avg_no_conf=avg_no_conf+no_obj_conf\n",
    "            avg_pos=avg_pos+pos_class\n",
    "            avg_neg=avg_neg+neg_class\n",
    "            total_loss=total_loss+loss.item()\n",
    "            avg_iou=avg_iou+iou\n",
    "            sys.stdout.write('\\rPgr:'+str(prg_counter/dataset_len*100*batch_size)+'%' ' L:'+ str(loss.item()))\n",
    "            sys.stdout.write(' IoU:' +str(iou)+' pob:'+str(conf)+ ' nob:'+str(no_obj_conf))\n",
    "            sys.stdout.write(' PCls:' +str(pos_class)+' ncls:'+str(neg_class))\n",
    "            sys.stdout.flush()\n",
    "            prg_counter=prg_counter+1\n",
    "            train_counter=train_counter+1\n",
    "        else:\n",
    "            print('missed')\n",
    "            print(strd.shape[0])\n",
    "            prg_counter=prg_counter+1\n",
    "        iou1=iou1.tolist()\n",
    "        iou_per_class= [0] * 80\n",
    "        averager_per_class=[1] * 80\n",
    "        for i,el in enumerate(iou1):\n",
    "            name=targets[i,5:].max(0)[1].cpu().detach().numpy()\n",
    "            iou_per_class[name]=iou_per_class[name]+el\n",
    "            averager_per_class[name]=averager_per_class[name]+1\n",
    "        for i in range(80):\n",
    "            if (iou_per_class[i]!=0):\n",
    "                if i<40:\n",
    "                    writer.add_scalar('Iou0/'+str(i), iou_per_class[i]/averager_per_class[i], train_counter)\n",
    "                else:\n",
    "                    writer.add_scalar('Iou1/'+str(i), iou_per_class[i]/averager_per_class[i], train_counter)\n",
    "        writer.add_scalar('AvLoss/train', total_loss/train_counter, train_counter)\n",
    "\n",
    "        \n",
    "        writer.add_scalar('AvIoU/train', avg_iou/train_counter, train_counter)\n",
    "\n",
    "        writer.add_scalar('AvPConf/train', avg_conf/train_counter, train_counter)\n",
    "        \n",
    "        writer.add_scalar('AvNConf/train', avg_no_conf/train_counter, train_counter)\n",
    "        \n",
    "        writer.add_scalar('AvClass/train', avg_pos/train_counter, train_counter)\n",
    "        \n",
    "        writer.add_scalar('AvNClass/train', avg_neg/train_counter, train_counter)\n",
    "        \n",
    "        del loss, raw_pred, targets, true_pred, images,iou,noobj_box,conf,iou1\n",
    "        torch.cuda.empty_cache()\n",
    "    if misses>0:\n",
    "        break\n",
    "#     torch.save(model.state_dict(), PATH)\n",
    "#     writer.add_scalar('Loss/train', total_loss/train_counter, e)\n",
    "#     writer.add_scalar('AIoU/train', avg_iou/train_counter, e)\n",
    "#     writer.add_scalar('PConf/train', avg_conf/train_counter, e)\n",
    "#     writer.add_scalar('NConf/train', avg_no_conf/train_counter, e)\n",
    "#     writer.add_scalar('PClass/train', avg_pos/train_counter, e)\n",
    "#     writer.add_scalar('NClass/train', avg_neg/train_counter, e)\n",
    "    print('\\ntotal number of misses is ' + str(misses))\n",
    "    print('\\n total average loss is '+str(total_loss/train_counter))\n",
    "    print('\\n total average iou is '+str(avg_iou/train_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.0001, 'batch_size': 16, 'weight_decay': 0.001, 'momentum': 0.9, 'optimizer': 'sgd', 'alpha': 0.5, 'gamma': 2, 'lcoord': 5, 'lno_obj': 1, 'iou_type': (0, 0, 0), 'iou_ignore_thresh': 0.5, 'tfidf': True, 'workers': 2, 'gpus': 2, 'path': 'pretrained', 'reduction': 'sum'}\n"
     ]
    }
   ],
   "source": [
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_step_count',\n",
       " 'add_param_group',\n",
       " 'defaults',\n",
       " 'load_state_dict',\n",
       " 'param_groups',\n",
       " 'state',\n",
       " 'state_dict',\n",
       " 'step',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "test=torch.tensor([[[1,2],[3,4]]])\n",
    "print(test.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0216, -0.1244],\n",
      "        [-0.5018,  0.0126],\n",
      "        [-0.3152, -0.0355],\n",
      "        [ 0.0959,  0.2126],\n",
      "        [-0.8653,  0.0340],\n",
      "        [-0.2147, -0.5316],\n",
      "        [-0.0711,  0.2382],\n",
      "        [-0.0854, -0.7116],\n",
      "        [-0.4368, -0.1484],\n",
      "        [-0.3921, -0.1823],\n",
      "        [-0.5624,  0.2302],\n",
      "        [ 0.0366,  0.0608],\n",
      "        [ 0.1972,  0.2450],\n",
      "        [ 0.0426,  0.0461],\n",
      "        [-1.4195, -0.7846],\n",
      "        [ 0.4011, -1.1072],\n",
      "        [-1.0203, -0.9770],\n",
      "        [-0.2076,  0.0770],\n",
      "        [-0.4059, -1.3375],\n",
      "        [-0.2784, -0.7123],\n",
      "        [-0.1746, -0.8052],\n",
      "        [-0.4275, -1.9284],\n",
      "        [-0.3326, -1.1696],\n",
      "        [ 0.0555, -1.6194],\n",
      "        [-0.4176, -0.3920],\n",
      "        [-0.4719, -0.7069],\n",
      "        [-0.1646,  0.3864],\n",
      "        [-0.5605, -1.2928],\n",
      "        [ 0.2676, -0.3395],\n",
      "        [ 0.2861,  0.1201],\n",
      "        [ 0.1030,  0.0109],\n",
      "        [-0.5617,  0.1186],\n",
      "        [ 0.2074,  0.5339],\n",
      "        [ 0.1418, -0.0217],\n",
      "        [ 0.3992,  0.0490],\n",
      "        [-0.2500,  0.1070],\n",
      "        [-0.4502,  0.0582],\n",
      "        [-0.2454,  0.5883],\n",
      "        [-0.5039, -0.0651],\n",
      "        [ 0.2944,  0.5497],\n",
      "        [ 0.0369,  0.1570]], device='cuda:0')\n",
      "tensor([0.0195, 0.0196, 0.0197, 0.0199, 0.0201, 0.0218, 0.0198, 0.0207, 0.0208,\n",
      "        0.0209, 0.0222, 0.0195, 0.0204, 0.0195, 0.0593, 0.0304, 0.0528, 0.0198,\n",
      "        0.0335, 0.0237, 0.0224, 0.0444, 0.0287, 0.0213, 0.0229, 0.0272, 0.0208,\n",
      "        0.0402, 0.0213, 0.0202, 0.0195, 0.0208, 0.0218, 0.0195, 0.0199, 0.0200,\n",
      "        0.0200, 0.0225, 0.0201, 0.0229, 0.0196], device='cuda:0')\n",
      "tensor([0.0178, 0.0183, 0.0188, 0.0195, 0.0200, 0.0237, 0.0192, 0.0216, 0.0218,\n",
      "        0.0221, 0.0242, 0.0177, 0.0210, 0.0176, 0.0485, 0.0329, 0.0458, 0.0192,\n",
      "        0.0353, 0.0264, 0.0246, 0.0419, 0.0315, 0.0228, 0.0253, 0.0301, 0.0217,\n",
      "        0.0396, 0.0228, 0.0203, 0.0175, 0.0219, 0.0235, 0.0178, 0.0194, 0.0199,\n",
      "        0.0199, 0.0247, 0.0202, 0.0252, 0.0182], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(targets[:,2:4])\n",
    "area=targets[:,2]*targets[:,3]\n",
    "weights=torch.abs(area.cuda())\n",
    "print(torch.softmax(weights,dim=0))\n",
    "weights=weights**(1/2)\n",
    "print(torch.softmax(weights,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  4.,  1.,  0.,  0.,  1.,\n",
      "         0.,  0.,  1.,  1.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,\n",
      "         2.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
      "         0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
      "[2, 4, 6, 8, 9, 2, 2, 6]\n",
      "39\n",
      "tensor([ 0, 38,  0,  0, 34,  0, 54, 54, 54, 54, 54, 63,  0,  0, 37, 37, 37, 37,\n",
      "         0,  0, 72, 56,  0, 44, 45, 46, 46, 60, 69, 22, 22,  0, 36, 57,  0, 65,\n",
      "        41, 73, 56], device='cuda:0')\n",
      "tensor([1.2657, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 2.9704, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.6636, 0.0000,\n",
      "        3.6636, 2.2773, 3.6636, 0.0000, 0.0000, 3.6636, 0.0000, 0.0000, 3.6636,\n",
      "        3.6636, 2.9704, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        2.0541, 0.0000, 2.9704, 3.6636, 0.0000, 0.0000, 3.6636, 0.0000, 0.0000,\n",
      "        3.6636, 0.0000, 3.6636, 0.0000, 0.0000, 0.0000, 3.6636, 0.0000, 0.0000,\n",
      "        3.6636, 3.6636, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0')\n",
      "tensor([1.2657, 3.6636, 1.2657, 1.2657, 3.6636, 1.2657, 2.0541, 2.0541, 2.0541,\n",
      "        2.0541, 2.0541, 3.6636, 1.2657, 1.2657, 2.2773, 2.2773, 2.2773, 2.2773,\n",
      "        1.2657, 1.2657, 3.6636, 2.9704, 1.2657, 3.6636, 3.6636, 2.9704, 2.9704,\n",
      "        3.6636, 3.6636, 2.9704, 2.9704, 1.2657, 3.6636, 3.6636, 1.2657, 3.6636,\n",
      "        3.6636, 3.6636, 2.9704], device='cuda:0')\n",
      "tensor([0.6328, 1.8318, 0.3164, 0.3164, 0.9159, 0.3164, 0.3424, 0.3424, 0.3424,\n",
      "        0.3424, 0.3424, 0.6106, 0.1582, 0.1582, 0.2847, 0.2847, 0.2847, 0.2847,\n",
      "        0.1582, 0.1582, 0.4071, 0.3300, 0.1406, 0.4071, 0.4071, 0.3300, 0.3300,\n",
      "        0.4071, 0.4071, 1.4852, 1.4852, 0.6328, 1.8318, 0.6106, 0.2109, 0.6106,\n",
      "        0.6106, 0.6106, 0.4951], device='cuda:0')\n",
      "tensor([0.0256, 0.0848, 0.0186, 0.0186, 0.0339, 0.0186, 0.0191, 0.0191, 0.0191,\n",
      "        0.0191, 0.0191, 0.0250, 0.0159, 0.0159, 0.0180, 0.0180, 0.0180, 0.0180,\n",
      "        0.0159, 0.0159, 0.0204, 0.0189, 0.0156, 0.0204, 0.0204, 0.0189, 0.0189,\n",
      "        0.0204, 0.0204, 0.0599, 0.0599, 0.0256, 0.0848, 0.0250, 0.0168, 0.0250,\n",
      "        0.0250, 0.0250, 0.0223], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "print(targets[:,5:].sum(axis=0))\n",
    "print(mask)\n",
    "print(sum(mask))\n",
    "classes=targets[:,5:].max(1)[1]\n",
    "print(classes)\n",
    "idf=torch.log(sum(mask)/targets[:,5:].sum(axis=0))\n",
    "idf[idf== float('inf')] = 0\n",
    "print(idf)\n",
    "print(idf[classes])\n",
    "\n",
    "tf=[1/mask[i] for i in range(len(mask)) for j in range(mask[i])]\n",
    "tf=torch.tensor([1/mask[i] for i in range(len(mask)) for j in range(mask[i])]).cuda()\n",
    "tfidf=tf*idf[classes]\n",
    "print(tfidf)\n",
    "\n",
    "print(torch.softmax(tfidf,dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets2,anchors,offset,strd,mask=helper.collapse_boxes(targets,pw_ph,cx_cy,stride)\n",
    "transformed_output=true_pred\n",
    "targets=targets\n",
    "offset=offset\n",
    "strd=strd\n",
    "mask=mask\n",
    "inp_dim\n",
    "'''\n",
    "this function takes the transformed_output and\n",
    "the target box in respect to the resized image size\n",
    "and returns a mask which can be applied to select the \n",
    "best raw input,anchors and cx_cy_offset\n",
    "and the noobj_mask for the negatives\n",
    "targets is a list\n",
    "'''\n",
    "#first transpose the centered normalised target coords\n",
    "centered_target=transpose_target(targets)[:,:,0:2]\n",
    "#multiply by inp_dim then devide by stride to get the relative grid size coordinates, floor the result to get the corresponding cell\n",
    "centered_target=torch.floor(centered_target*inp_dim/strd)\n",
    "#create a mask to find where the gt falls into which gridcell in the grid coordinate system\n",
    "fall_into_mask=centered_target==offset\n",
    "fall_into_mask=fall_into_mask[:,:,0]&fall_into_mask[:,:,1]\n",
    "#     fall_into_mask= ~fall_into_mask\n",
    "#create a copy of the transformed output\n",
    "best_bboxes=transformed_output.clone()\n",
    "#apply reverse mask to copy in order to zero all other bbox locations\n",
    "best_bboxes[~fall_into_mask]=0   \n",
    "#transform the copy to xmin,xmax,ymin,ymax\n",
    "best_responsible_coord=get_abs_coord(best_bboxes)\n",
    "targets=transpose_target(get_abs_coord(targets))*inp_dim\n",
    "#calculate best iou and mask\n",
    "responsible_iou=bbox_iou(best_responsible_coord,targets,True)\n",
    "\n",
    "responsible_iou[responsible_iou.ne(responsible_iou)] = 0\n",
    "responsible_mask=responsible_iou.max(dim=0)[0] == responsible_iou\n",
    "\n",
    "print(responsible_mask.shape)\n",
    "\n",
    "abs_coord=get_abs_coord(transformed_output)\n",
    "iou=bbox_iou(abs_coord,targets,True)\n",
    "iou[iou.ne(iou)] = 0\n",
    "ignore_mask=0.5<=iou\n",
    "inverted_mask=iou.max(dim=0)[0] != iou\n",
    "noobj_mask=~same_picture_mask(responsible_mask.clone()|ignore_mask,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(responsible_mask)\n",
    "if(responsible_mask.sum()>sum(mask)):\n",
    "    print('jello')\n",
    "    responsible_mask1=correct_iou_mask(responsible_mask,fall_into_mask)\n",
    "print(responsible_mask1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.randint(0, 9, (1,)) == torch.arange(9)\n",
    "a=a.repeat(5)\n",
    "a=a.repeat(2,1)\n",
    "print(a)\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={'lr':0.01,'batch_size':16,'weight_decay':0.001,'optimizer':'sgd','alpha':0.5,'gamma':2,'iou_ignore_thresh':0.5,'tfidf':True,'reduction':'sum'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_mask=(responsible_mask.sum(axis=0)==responsible_mask.sum(axis=0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsible_mask[:,le_mask]=(responsible_mask[:,le_mask]&fall_into_mask[le_mask,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(le_mask)\n",
    "print(fall_into_mask[le_mask,:].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([1,2,3,4,5,6,6,6])\n",
    "indices=(((a==a.max())==True).nonzero())\n",
    "for ind in indices:\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "testing with test.pth\n",
      "\n",
      "Length of dataset is 5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/konsa15/.conda/envs/faster_rcnn/lib/python3.8/site-packages/torch/nn/functional.py:2503: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    }
   ],
   "source": [
    "from darknet import *\n",
    "import darknet as dn\n",
    "import util as util\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import timeit\n",
    "from dataset import *\n",
    "import torchvision.ops.boxes as nms_box\n",
    "import helper as helper\n",
    "from utils import *\n",
    "from BoundingBox import BoundingBox\n",
    "from BoundingBoxes import BoundingBoxes\n",
    "\n",
    "\n",
    "net = Darknet(\"../cfg/yolov3.cfg\")\n",
    "inp_dim=net.inp_dim\n",
    "pw_ph=net.pw_ph\n",
    "cx_cy=net.cx_cy\n",
    "stride=net.stride\n",
    "\n",
    "\n",
    "'''\n",
    "when loading weights from dataparallel model then, you first need to instatiate the dataparallel model \n",
    "if you start fresh then first model.load_weights and then make it parallel\n",
    "'''\n",
    "try:\n",
    "    PATH = 'test.pth'\n",
    "    weights = torch.load(PATH)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 3:\n",
    "        print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(net)\n",
    "    else:\n",
    "        model=net\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "except FileNotFoundError: \n",
    "    net.load_weights(\"../yolov3.weights\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "      print(\"Using \", torch.cuda.device_count(), \"GPUs!\")\n",
    "      model = nn.DataParallel(net)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('testing with '+ PATH +'\\n')\n",
    "transformed_dataset=Coco(partition='val',\n",
    "                                           transform=transforms.Compose([\n",
    "                                            ResizeToTensor(inp_dim)\n",
    "                                           ]))\n",
    "\n",
    "\n",
    "\n",
    "dataset_len=(len(transformed_dataset))\n",
    "print('Length of dataset is '+ str(dataset_len)+'\\n')\n",
    "batch_size=8\n",
    "\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                        shuffle=True,collate_fn=helper.my_collate, num_workers=2)\n",
    "\n",
    "true_pos=0\n",
    "false_pos=0\n",
    "counter=0\n",
    "iou_threshold=0.5\n",
    "confidence=0.01\n",
    "recall_counter=0\n",
    "\n",
    "for images,targets,img_name in dataloader:\n",
    "    inp=images.cuda()\n",
    "    raw_pred = model(inp, torch.cuda.is_available())\n",
    "    targets,anchors,offset,strd,mask=helper.collapse_boxes(targets,pw_ph,cx_cy,stride)\n",
    "    raw_pred=raw_pred.to(device='cuda')\n",
    "    true_pred=util.transform(raw_pred.clone(),pw_ph,cx_cy,stride)\n",
    "    \n",
    "    classes=true_pred[:,:,5:].max(2)[0]\n",
    "    objectness=true_pred[:,:,4]\n",
    "    \n",
    "    sorted_pred=torch.sort(objectness,descending=True)\n",
    "    pred_mask=sorted_pred[0]>confidence\n",
    "    indices=[(sorted_pred[1][e,:][pred_mask[e,:]]) for e in range(pred_mask.shape[0])]\n",
    "    pred_final=[true_pred[i,indices[i],:] for i in range(len(indices))]\n",
    "    \n",
    "    pred_final_coord=[util.get_abs_coord(pred_final[i].unsqueeze(-2)) for i in range(len(pred_final))]\n",
    "    \n",
    "    indices=[nms_box.nms(pred_final_coord[i][0],pred_final[i][:,4],iou_threshold) for i in range(len(pred_final))]\n",
    "\n",
    "    pred_final=[pred_final[i][indices[i],:] for i in range(len(pred_final))]\n",
    "#     pred_final[:,0:4]=pred_final[:,0:4]/inp_dim\n",
    "    helper.write_pred(img_name,pred_final,inp_dim)\n",
    "    \n",
    "\n",
    "    \n",
    "# Read txt files containing bounding boxes (ground truth and detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darknet import *\n",
    "import darknet as dn\n",
    "import util as util\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import timeit\n",
    "from dataset import *\n",
    "import torchvision.ops.boxes as nms_box\n",
    "import helper as helper\n",
    "from utils import *\n",
    "from BoundingBox import BoundingBox\n",
    "from BoundingBoxes import BoundingBoxes\n",
    "boundingboxes = helper.getBoundingBoxes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COCO map is: 0.6196015267338008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-af3802717a1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mMMap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmetricsPerClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetPascalVOCMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboundingboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOUThreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m#     print(\"Average precision values per class:\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Loop through classes to obtain their metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data1/users/konsa15/workspace/notebooks/coco/yolo/Evaluator.py\u001b[0m in \u001b[0;36mGetPascalVOCMetrics\u001b[0;34m(self, boundingboxes, IOUThreshold, method)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# print('dect %s => %s' % (dects[d][0], dects[d][3],))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Find ground truth image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0miouMax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data1/users/konsa15/workspace/notebooks/coco/yolo/Evaluator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# print('dect %s => %s' % (dects[d][0], dects[d][3],))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Find ground truth image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0miouMax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Evaluator import *\n",
    "from utils import *\n",
    "from BoundingBox import BoundingBox\n",
    "from BoundingBoxes import BoundingBoxes\n",
    "\n",
    "\n",
    "boundingboxes = helper.getBoundingBoxes()\n",
    "evaluator = Evaluator()\n",
    "\n",
    "iou=0.5\n",
    "MMap=0\n",
    "while iou<1:\n",
    "    metricsPerClass = evaluator.GetPascalVOCMetrics(boundingboxes, IOUThreshold=iou)\n",
    "#     print(\"Average precision values per class:\\n\")\n",
    "    # Loop through classes to obtain their metrics\n",
    "    mAP=0\n",
    "    counter=0\n",
    "    for mc in metricsPerClass:\n",
    "        # Get metric values per each class\n",
    "        c = mc['class']\n",
    "        precision = mc['precision']\n",
    "        recall = mc['recall']\n",
    "        average_precision = mc['AP']\n",
    "        ipre = mc['interpolated precision']\n",
    "        irec = mc['interpolated recall']\n",
    "        # Print AP per class\n",
    "        mAP=average_precision+mAP\n",
    "#         print('%s: %f' % (c, average_precision))\n",
    "    MMap=MMap+mAP/80\n",
    "    iou=iou+0.05\n",
    "    print('\\nCOCO map is:',MMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.001576\n",
      "1: 0.000500\n",
      "10: 0.000194\n",
      "11: 0.000000\n",
      "12: 0.000000\n",
      "13: 0.003635\n",
      "14: 0.000061\n",
      "15: 0.003767\n",
      "16: 0.007893\n",
      "17: 0.001253\n",
      "18: 0.002096\n",
      "19: 0.003806\n",
      "2: 0.001995\n",
      "20: 0.002062\n",
      "21: 0.019332\n",
      "22: 0.005166\n",
      "23: 0.001213\n",
      "24: 0.000123\n",
      "25: 0.000791\n",
      "26: 0.001165\n",
      "27: 0.000106\n",
      "28: 0.000452\n",
      "29: 0.006250\n",
      "3: 0.000064\n",
      "30: 0.000189\n",
      "31: 0.000000\n",
      "32: 0.005474\n",
      "33: 0.002095\n",
      "34: 0.001349\n",
      "35: 0.000322\n",
      "36: 0.001117\n",
      "37: 0.000121\n",
      "38: 0.000508\n",
      "39: 0.000694\n",
      "4: 0.003233\n",
      "40: 0.002795\n",
      "41: 0.002764\n",
      "42: 0.000034\n",
      "43: 0.000083\n",
      "44: 0.000000\n",
      "45: 0.005726\n",
      "46: 0.001819\n",
      "47: 0.000814\n",
      "48: 0.000644\n",
      "49: 0.004769\n",
      "5: 0.007732\n",
      "50: 0.000000\n",
      "51: 0.000424\n",
      "52: 0.000000\n",
      "53: 0.009787\n",
      "54: 0.001989\n",
      "55: 0.000344\n",
      "56: 0.000159\n",
      "57: 0.008519\n",
      "58: 0.000000\n",
      "59: 0.003829\n",
      "6: 0.002898\n",
      "60: 0.001310\n",
      "61: 0.004003\n",
      "62: 0.001842\n",
      "63: 0.002337\n",
      "64: 0.009707\n",
      "65: 0.001246\n",
      "66: 0.000000\n",
      "67: 0.009142\n",
      "68: 0.000000\n",
      "69: 0.000194\n",
      "7: 0.002832\n",
      "70: 0.000000\n",
      "71: 0.003943\n",
      "72: 0.002155\n",
      "73: 0.000207\n",
      "74: 0.000342\n",
      "75: 0.000999\n",
      "76: 0.002137\n",
      "77: 0.000277\n",
      "78: 0.000000\n",
      "79: 0.000000\n",
      "8: 0.000155\n",
      "9: 0.000774\n"
     ]
    }
   ],
   "source": [
    "0.6403413112473646\n",
    "0.6503946905414258\n",
    "for mc in metricsPerClass:\n",
    "    c = mc['class']\n",
    "    average_precision = mc['AP']\n",
    "    print('%s: %f' % (c, average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3651112345706164\n"
     ]
    }
   ],
   "source": [
    "print(MMap/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9: 0.000231\n"
     ]
    }
   ],
   "source": [
    "print('%s: %f' % (c, average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f162f914160>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluator import *\n",
    "evaluator = Evaluator()\n",
    "metricsPerClass = evaluator.GetPascalVOCMetrics(boundingboxes, IOUThreshold=0.5)\n",
    "print(\"Average precision values per class:\\n\")\n",
    "# Loop through classes to obtain their metrics\n",
    "mAP=0\n",
    "counter=0\n",
    "for mc in metricsPerClass:\n",
    "    # Get metric values per each class\n",
    "    c = mc['class']\n",
    "    precision = mc['precision']\n",
    "    recall = mc['recall']\n",
    "    average_precision = mc['AP']\n",
    "    ipre = mc['interpolated precision']\n",
    "    irec = mc['interpolated recall']\n",
    "    # Print AP per class\n",
    "    mAP=average_precision+mAP\n",
    "    print('%s: %f' % (c, average_precision))\n",
    "\n",
    "print('map is:',mAP/80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.07290217209136615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "coord=pred_final[:,:4].cpu().detach().numpy()\n",
    "conf=pred_final[:,4:5].cpu().detach().numpy()\n",
    "mat=np.hstack((conf,coord))\n",
    "\n",
    "classes=pred_final[:,5:].max(1)[1].cpu().detach().numpy()\n",
    "classes=np.array([classes]).T\n",
    "\n",
    "mat=np.hstack((classes,mat))\n",
    "mat=np.array(mat)\n",
    "\n",
    "df=pd.DataFrame(mat,index=None,columns=None)\n",
    "df[0]=df[0].apply(lambda x: int(x))\n",
    "\n",
    "df.to_csv('test.txt',sep=' ',header=False,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('test.txt',mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([1,2,2,3,4,2,3,1,4])\n",
    "print(a.max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=(inp).squeeze(0)\n",
    "image=np.array(image.cpu())\n",
    "print(image.shape)\n",
    "image =  image[:,:,::-1].transpose((1,2,0))\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "all_files = glob.glob('plots/' + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in sorted(all_files):\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=1, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# gca stands for 'get current axis'\n",
    "\n",
    "title_list=['AIoU_train','Loss_train','NClass_train','NConf_train','PClass','PConf']\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "fig.suptitle('KL for xy loss')\n",
    "fig.subplots_adjust(hspace=0.3, wspace=-.6)\n",
    "colors = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c']\n",
    "i=2\n",
    "k=0\n",
    "while i <18:\n",
    "    ax = fig.add_subplot(2, 9, i)\n",
    "    frame.plot(x =1 , y = i,ax=ax,legend=False)\n",
    "    ax.set_title(title_list[k])\n",
    "    i=i+3\n",
    "    k=k+1\n",
    "plt.savefig('original.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
